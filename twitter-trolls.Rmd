---
title: "Exploring 2016 Twitter Trolls"
author: "Benjamin Ortiz Ulloa"
date: "1/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message = F}
library(tidyverse)
library(igraph)
```

At last year's Data Days Texas, Will Lyon spoke about [Neo4j's findings with regards to the 2016 Russian Twitter Trolls](https://neo4j.com/blog/story-behind-russian-twitter-trolls/). They were able to do this analysis on the data [NBC collected](https://www.kaggle.com/vikasg/russian-troll-tweets). Let's explore the same data set and see what we can learn.

```{r, message=F}
tweets <- read_csv('russian-troll-tweets/tweets.csv')
```

To start off we'll only need the users' identification key and their hashtag information. We'll also need the text of the tweets for future referece. 

```{r}
tweet_hashtag <- tweets %>% 
  filter(hashtags != '[]') %>% #this represents a tweet with no hashtags
  mutate(hashtags = map(hashtags, jsonlite::fromJSON)) %>% #the stored info is a json file
  select(user_key, hashtags, text) %>% 
  unnest() 

head(tweet_hashtag)
```

Since we are only working with an edge list, we should create a directed graph in which people point to hashtags. This is useful since we can infer a node type by where it lies on a given edge.

```{r}
# user -[used hashtag]-> hashtag
tweet_hashtag_edges <- tweet_hashtag %>%
  select(-text) %>%
  count(user_key, hashtags, sort = T) %>% #aggregatign user-hashtag occurances
  rename(weight = n) %>%
  mutate(type = 'used_hashtag',
         user_key = paste0('@', user_key), #prevent duplicate node names
         hashtags = paste0('#', hashtags) #prevent duplicate node names
         ) 

head(tweet_hashtag_edges)
```

We now have an edge list with enough information to get started. We should give the nodes plotting attributes so we can plot the graph later.

```{r}
tweet_hashtag_nodes <- bind_rows(
  tibble(
    label = tweet_hashtag_edges$user_key %>% unique(),
    type = 'user',
    color = 'purple',
    size = 3
  ),
  tibble(
    label = tweet_hashtag_edges$hashtags %>% unique(),
    type = 'hashtag',
    color = 'lightgrey',
    size = 3
  )
)

```

```{r}
tweet_hashtag_g <- tweet_hashtag_edges %>%
  graph_from_data_frame(vertices = tweet_hashtag_nodes)

tweet_hashtag_g
```

This is a big graph. This will look like a gian hair ball if we plot it. Also, we are only interested in frequent person-hashtag connections. Let's take a look  at the weight distribution.

```{r, out.width = '1000px'}
summary(E(tweet_hashtag_g)$weight)

# let's filter the graph down 
tweet_hashtag_g_filtered <- tweet_hashtag_g %>%
  {. - E(.)[weight <= 5]} %>%
  {. - V(.)[degree(.) == 0]}

plot(tweet_hashtag_g_filtered, 
     vertex.label = '',
     edge.arrow.mode = '-')
```

From the looks of it, the is not a complete graph. That is, not every node can be connected with every other node. This shows that their are different groups, or component. Let's take a look at the differentcomponents

```{r}

tweet_hashtag_g_filtered_components <- components(tweet_hashtag_g_filtered)

names(tweet_hashtag_g_filtered_components)

tweet_hashtag_g_filtered_components %>% 
  groups() %>%
  map(head, n = 10)
```

The smaller components are made up of just one user tweeting about a topic very frequently. Let's dig deeper into this information and see how the hashtags relate to eachother. Hashtags are connected to other hashtags by people who use them. If many people use the same two hashtags, then we can safely assume that they are related in some way.

```{r, message=F}
tweet_hashtag_g_filtered %>%
  {
    V(.)$type <- V(.)$type == 'user'; #boolean type is necessary for bipartite projection
    V(.)$degree <- degree(.);
    V(.)$weighted_degree <- strength(.);
    .
  } %>%
  igraph::bipartite_projection() %>%
  {
    . <- map(., function(x){
      V(x)$component <- components(x)$membership;
      return(x)
    })
    hashtag_g <<- .$proj1; #projection 1 is type FALSE
    user_g <<- .$proj2; #projection 2 is type TRUE
  }

set.seed(4321)
plot(hashtag_g, vertex.size = 3, vertex.label = '')

```

There is a large component within the tweets. We can distiguish them by using a community detection algorithm and marking the nodes according to their community. The **walktrap algorthm** uses a series of random walks to determine who are in the same community.


```{r}
hashtag_communities <- walktrap.community(hashtag_g)

hashtag_communities %>% 
  groups %>% 
  map(function(grp){str_replace(grp, '^[^_]+_', '') %>% head(10)} )
```

While some of these tweets look somewhat normal, they take on a completely different tone once we understand that they come from a troll.

```{r}
tweet_hashtag %>%
  filter(hashtags == 'Texit')
  # filter(hashtags == 'BTP')
  # filter(hashtags == 'PodernFamily')
```

We can mark the nodes according to the community algorithm. The order of membership information returned from the algorithm corresponds to the order of the vertices of the graph used.

```{r}
V(hashtag_g)$membership <- hashtag_communities$membership

vertex_clr_palette <- rainbow(hashtag_communities %>% groups %>% length()) #simple color palette

set.seed(4321)
hashtag_g %>%
  plot(
    vertex.color = map_chr(V(.)$membership, function(x){vertex_clr_palette[x]}),
    vertex.label = ''
  )
```

We can create super nodes that represent the community's discovered. The edges between the super nodes would represent users who tweeted hashtags from both.

```{r}
hashtag_community_summary <- hashtag_g %>%
  {
    E(.)$tailCommunity <- tail_of(., E(.))$membership; #store node information in edges 
    E(.)$headCommunity <- head_of(., E(.))$membership; #to refrence later
    .
  } %>%
  as_data_frame('both') %>%
  {
    el <- .$edges %>%
      filter(tailCommunity != headCommunity) %>%
      #1 - 6 is same as 6 - 1
      mutate(
        tail = map2_dbl(tailCommunity, headCommunity, min) %>% round %>% as.character(),
        head = map2_dbl(tailCommunity, headCommunity, max) %>% round %>% as.character()
      ) %>%
      group_by(tail, head) %>%
      nest() %>%
      mutate(data = map(data, function(x){
        top_hashes <- x %>%
          arrange(desc(weight)) %>%
          head(3) %>%
          {paste(.$from, .$to, sep = ' | ', collapse = '\n')} %>%
          str_replace_all('hashtag_', '')
        
        tibble(
          top_hashes,
          sum_weight = sum(x$weight),
          ave_weight = mean(x$weight),
          count = nrow(x)
        )
      })) %>% 
      unnest()
    
    nl <- .$vertices %>%
      mutate(membership = as.character(membership)) %>%
      group_by(membership) %>%
      nest() %>%
      mutate(top_hashes = map_chr(data, function(x){
        x %>%
          head %>%
          .$name %>%
          paste(collapse = '\n') %>%
          str_replace_all('hashtag_', '')
      }))
    
    set.seed(4321)
    graph_from_data_frame(el, F, nl) %>%
      {
        l <- layout_nicely(.) #store layout information in vertices
        V(.)$x <- l[,1]
        V(.)$y <- l[,2]
        .
      }
  } 

```


```{r}
hashtag_community_summary %>% 
  plot(vertex.color = map_chr(V(.)$name, function(x){vertex_clr_palette[as.numeric(x)]}))
```


```{r, out.width = '1000px', out.height='1000px'}
hashtag_community_summary %>%
  plot(
    vertex.label = V(.)$top_hashes,
    vertex.label.cex = .5,
    vertex.shape = 'none',
    main = 'Twitter Troll Groups: Hashtags Common to Group',
    asp = 0) 
```

```{r, out.width = '1000px', out.height='1000px'}
hashtag_community_summary %>%
  {. - V(.)[degree(.) == 0]} %>%
  plot(
    edge.label = E(.)$top_hashes,
    edge.label.cex = .5,
    vertex.size = 5,
    main = "Twitter Troll Groups: Hashtags Connecting two Groups",
    asp = 0
  )
```


## Text Analysis

[Please take a look at the corresponding slides before continuing](img/Modeling & Graphing Text.pdf)

The theme of tagging a node is important because we don't always have direct connections to determine if two nodes are related. Community detection relies on the network's structure. Luckily, we have text stored in the nodes so another option that we have for identifying nodes is topic modelling. 

```{r}
library(tidytext)
library(stm)
```

Topic modelling typically requires a form of document-term matrix. This is simply a matrix that counts the occurance of a particular word in a particular document. This requires us to figure out to extract word frequencies. This would normally involve tokenization and the removal of stop word's, but another beautiful thing about working with twitter data is that the hashtags themselves are tokenized words. 

```{r}
#https://juliasilge.com/blog/evaluating-stm/
tidy_hash <- tweet_hashtag %>%
  select(-text) %>%
  count(user_key, hashtags, sort = T)

head(tidy_hash)
```

Once we have the term frequencies, we can store the counts in the intersection of the words and documents.

```{r, message=F}
tweets_sparse_hash <- tidy_hash %>%
  cast_sparse(user_key, hashtags, n)

tweets_sparse_hash[1:5, 1:5]
```

We can then use the dtm to run a topic model. There are many topic modelling algorithms - a common one is the latent Dirichlet allocation - however, here we use a structural topic model. These algorithms genreally take some time 

```{r}
# topic_model_hash <- stm(tweets_sparse_hash, K = 6, 
#                         verbose = FALSE, init.type = "Spectral") 


topic_model_hash <- read_rds('created_data/twitter_troll_tweets_user_hash.rds')
topic_model_hash
```

One of the return values of the STM model is a beta value which tells us the probability a word belongs to a topic. 

```{r, out.width = '1000px', out.height= '1000px'}
td_beta_hash <- tidy(topic_model_hash)

td_beta_hash %>%
  filter(term != 'rt') %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE, color = 'black') +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  labs(x = NULL, y = expression(beta),
       title = "Grouping of Hashtags: Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") +
  scale_fill_brewer(type = 'qual') +
  theme_bw()
```

We also have a gamma, which tells us the probability that a given document (or, in our case, troll) belongs to a particular topic.

```{r}
td_gamma_hash <- tidy(topic_model_hash, matrix = "gamma",
                      document_names = rownames(tweets_sparse_hash)) %>%
  arrange(document)

categorize_user <- td_gamma_hash %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  mutate(color = map_chr(topic, function(x){scales::brewer_pal('qual')(6)[x]})) %>%
  rename(name = document)

categorize_user

colored_user_g <- user_g %>% 
  as_data_frame('both') %>%
  {
    el <- .$edges %>%
      mutate(
        from = str_remove(from, '^@'),
        to = str_remove(to, '^@')
      ) 
    
    nl <- .$vertices %>%
      remove_rownames() %>%
      select(-color) %>%
      mutate(
        name = str_remove(name, '^@')
      ) %>%
      left_join(categorize_user, by = "name") 
    
    graph_from_data_frame(el, F, nl)
  }

plot(colored_user_g, vertex.label = '')
```
