---
title: "Exploring 2016 Twitter Trolls"
author: "Benjamin Ortiz Ulloa"
date: "1/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message = F}
library(tidyverse)
library(igraph)
```

```{r, message=F}
tweets <- read_csv('russian-troll-tweets/tweets.csv')
users <- read_csv('russian-troll-tweets/users.csv')
```

```{r}
tweet_hashtag <- tweets %>% 
  filter(hashtags != '[]') %>%
  mutate(hashtags = map(hashtags, jsonlite::fromJSON)) %>%
  select(user_key, hashtags, text) %>%
  unnest() 

head(tweet_hashtag)
```

```{r}
# user -[used hashtag]-> hashtag
tweet_hashtag_edges <- tweet_hashtag %>%
  select(-text) %>%
  count(user_key, hashtags, sort = T) %>%
  rename(weight = n) %>%
  mutate(type = 'used_hashtag',
         user_key = paste0('@', user_key), #prevent duplicate node names
         hashtags = paste0('#', hashtags) #prevent duplicate node names
         ) 

tweet_hashtag_edges
```

```{r}
tweet_hashtag_nodes <- bind_rows(
  tibble(
    label = tweet_hashtag_edges$user_key %>% unique(),
    type = 'user',
    color = 'purple',
    size = 3
  ),
  tibble(
    label = tweet_hashtag_edges$hashtags %>% unique(),
    type = 'hashtag',
    color = 'lightgrey',
    size = 3
  )
)

set.seed(1234)
sample_n(tweet_hashtag_nodes, 10) 
```

```{r}
tweet_hashtag_g <- tweet_hashtag_edges %>%
  graph_from_data_frame(vertices = tweet_hashtag_nodes)

tweet_hashtag_g
```

```{r}
summary(E(tweet_hashtag_g)$weight)

tweet_hashtag_g_filtered <- tweet_hashtag_g %>%
  {. - E(.)[weight <= 5]} %>%
  {. - V(.)[degree(.) == 0]}

plot(tweet_hashtag_g_filtered, 
     vertex.label = '',
     edge.arrow.mode = '-')
```

```{r}
tweet_hashtag_g_filtered_components <- components(tweet_hashtag_g_filtered)

names(tweet_hashtag_g_filtered_components)

tweet_hashtag_g_filtered_components %>% 
  groups() %>%
  map(head, n = 10)
```

```{r, message=F}
tweet_hashtag_g_filtered %>%
  {
    V(.)$type <- V(.)$type == 'user'; #boolean type is necessary for bipartite projection
    V(.)$degree <- degree(.);
    V(.)$weighted_degree <- strength(.);
    .
  } %>%
  igraph::bipartite_projection() %>%
  {
    . <- map(., function(x){
      V(x)$component <- components(x)$membership;
      return(x)
    })
    hashtag_g <<- .$proj1; #projection 1 is type FALSE
    user_g <<- .$proj2; #projection 2 is type TRUE
  }
```

```{r}
hashtag_communities <- walktrap.community(hashtag_g)

hashtag_communities %>% 
  groups %>% 
  map(function(grp){str_replace(grp, '^[^_]+_', '') %>% head(10)} )
```

```{r}
tweet_hashtag %>%
  filter(hashtags == 'Texit')
  # filter(hashtags == 'BTP')
  # filter(hashtags == 'PodernFamily')
```

```{r}
V(hashtag_g)$membership <- hashtag_communities$membership

vertex_clr_palette <- rainbow(hashtag_communities %>% groups %>% length())

hashtag_g %>%
  plot(
    vertex.color = map_chr(V(.)$membership, function(x){vertex_clr_palette[x]}),
    vertex.label = ''
  )
```

```{r}
hashtag_community_summary <- hashtag_g %>%
  {
    E(.)$tailCommunity <- tail_of(., E(.))$membership;
    E(.)$headCommunity <- head_of(., E(.))$membership;
    .
  } %>%
  as_data_frame('both') %>%
  {
    el <- .$edges %>%
      filter(tailCommunity != headCommunity) %>%
      #1 - 6 is same as 6 - 1
      mutate(
        tail = map2_dbl(tailCommunity, headCommunity, min) %>% round %>% as.character(),
        head = map2_dbl(tailCommunity, headCommunity, max) %>% round %>% as.character()
      ) %>%
      group_by(tail, head) %>%
      nest() %>%
      mutate(data = map(data, function(x){
        top_hashes <- x %>%
          arrange(desc(weight)) %>%
          head(3) %>%
          {paste(.$from, .$to, sep = ' | ', collapse = '\n')} %>%
          str_replace_all('hashtag_', '')
        
        tibble(
          top_hashes,
          sum_weight = sum(x$weight),
          ave_weight = mean(x$weight),
          count = nrow(x)
        )
      })) %>% 
      unnest()
    
    nl <- .$vertices %>%
      mutate(membership = as.character(membership)) %>%
      group_by(membership) %>%
      nest() %>%
      mutate(top_hashes = map_chr(data, function(x){
        x %>%
          head %>%
          .$name %>%
          paste(collapse = '\n') %>%
          str_replace_all('hashtag_', '')
      }))
    
    set.seed(4321)
    graph_from_data_frame(el, F, nl) %>%
      {
        l <- layout_nicely(.)
        V(.)$x <- l[,1]
        V(.)$y <- l[,2]
        .
      }
  } 

```

```{r}
hashtag_community_summary %>% 
  plot(vertex.color = map_chr(V(.)$name, function(x){vertex_clr_palette[as.numeric(x)]}))
```


```{r, out.width = '1000px', out.height='1000px'}
hashtag_community_summary %>%
  plot(
    vertex.label = V(.)$top_hashes,
    vertex.label.cex = .5,
    vertex.shape = 'none',
    main = 'Twitter Troll Groups: Hashtags Common to Group',
    asp = 0) 
```

```{r, out.width = '1000px', out.height='1000px'}
hashtag_community_summary %>%
  {. - V(.)[degree(.) == 0]} %>%
  plot(
    edge.label = E(.)$top_hashes,
    edge.label.cex = .5,
    vertex.size = 5,
    main = "Twitter Troll Groups: Hashtags Connecting two Groups",
    asp = 0
  )
```


## Text Analysis

```{r}
library(tidytext)
library(stm)
```

```{r}
#https://juliasilge.com/blog/evaluating-stm/

tidy_tweets <- tweet_hashtag %>%
  select(-hashtags) %>%
  distinct() %>%
  mutate(text = str_replace_all(text, "&#x27;|&quot;|&#x2F;", "'"), ## weird encoding
         text = str_replace_all(text, "<a(.*?)>", " "),             ## links 
         text = str_replace_all(text, "&gt;|&lt;|&amp;", " "),      ## html yuck
         text = str_replace_all(text, "&#[:digit:]+;", " "),        ## html yuck
         text = str_remove_all(text, "<[^>]*>"),                    ## mmmmm, more html yuck
         text = str_remove_all(text, 'http\\S+')) %>%
  filter(!is.na(text)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  anti_join(get_stopwords(), by = 'word') %>%
  filter(!str_detect(word, "[0-9]+")) %>%
  filter(word != 'rt')%>%
  count(user_key, word, sort = T)

head(tidy_tweets)
```

```{r}
tweets %>% 
  # filter(user_key == 'ameliebaldwin') %>% 
  filter(user_key == 'newspeakdaily') %>%
  select(text) %>%
  filter(!str_detect(text, '^RT')) %>%
  head()
```

```{r, message = F}
tweets_sparse <- tidy_tweets %>%
  cast_sparse(user_key, word, n)

tweets_sparse[1:5, 1:5]

```

```{r}
# topic_model <- stm(tweets_sparse, K = ,
#                    verbose = FALSE, init.type = "Spectral")

topic_model <- read_rds('created_data/twitter_troll_tweets_users.rds')

topic_model

td_beta <- tidy(topic_model)

td_beta
```

```{r}
td_beta %>%
  filter(term != 'rt') %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  group_by(topic) %>%
  nest() %>%
  mutate(data, imap(data, function(x, i){
    x %>%
      mutate(term = paste0(paste(rep(' ', i), collapse = '' ), term))
  })) %>%
  unnest()  %>%
  arrange(desc(beta)) %>%
  mutate(topic = paste0("Topic ", topic)) %>%
  ggplot(aes(factor(term, rev(unique(term))), beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  labs(x = NULL, y = expression(beta),
       title = "Grouping of Users: Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")
```


```{r}
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(tweets_sparse)) %>%
  arrange(document) 

head(td_gamma, 12)

categorize_users <- td_gamma %>%
  group_by(document) %>%
  top_n(1, gamma)

head(categorize_users)
```

```{r}
tidy_hash <- tweet_hashtag %>%
  select(-text) %>%
  count(user_key, hashtags, sort = T)

head(tidy_hash)
```

```{r, message=F}
tweets_sparse_hash <- tidy_hash %>%
  cast_sparse(user_key, hashtags, n)

tweets_sparse_hash[1:5, 1:5]
```

```{r}
# topic_model_hash <- stm(tweets_sparse_hash, K = 6, 
#                         verbose = FALSE, init.type = "Spectral")


topic_model_hash <- read_rds('created_data/twitter_troll_tweets_user_hash.rds')
topic_model_hash
```


```{r}
td_beta_hash <- tidy(topic_model_hash)

td_beta_hash %>%
  filter(term != 'rt') %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic)) %>%
  # group_by(topic) %>%
  # nest() %>%
  # mutate(data, imap(data, function(x, i){
  #   x %>%
  #     mutate(term = paste0(paste(rep(' ', i), collapse = '' ), term))
  # })) %>%
  # unnest()  %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE, color = 'black') +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  labs(x = NULL, y = expression(beta),
       title = "Grouping of Hashtags: Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics") +
  scale_fill_brewer(type = 'qual') +
  theme_bw()
```

```{r}
td_gamma_hash <- tidy(topic_model_hash, matrix = "gamma",
                      document_names = rownames(tweets_sparse_hash)) %>%
  arrange(document)

categorize_user <- td_gamma_hash %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  mutate(color = map_chr(topic, function(x){scales::brewer_pal('qual')(6)[x]})) %>%
  rename(name = document)

colored_user_g <- user_g %>% 
  as_data_frame('both') %>%
  {
    el <- .$edges %>%
      mutate(
        from = str_remove(from, '^@'),
        to = str_remove(to, '^@')
      ) 
    
    nl <- .$vertices %>%
      remove_rownames() %>%
      select(-color) %>%
      mutate(
        name = str_remove(name, '^@')
      ) %>%
      left_join(categorize_user, by = "name") 
    
    graph_from_data_frame(el, F, nl)
  }

plot(colored_user_g, vertex.label = '')
```